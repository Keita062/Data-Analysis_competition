{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c5348aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: mecab-python3 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.12)\n",
      "Requirement already satisfied: ipadic in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: openai in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.11.1)\n",
      "Requirement already satisfied: sniffio in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas mecab-python3 ipadic scikit-learn matplotlib seaborn openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "26804f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.2.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from rank_bm25) (2.2.3)\n",
      "Collecting transformers<6.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.10.0-cp313-cp313-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Collecting shellingham (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (78.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Collecting regex!=2019.12.17 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<6.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (0.14.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from typer-slim->huggingface-hub>=0.20.0->sentence-transformers) (8.1.8)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\管理\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.20.0->sentence-transformers) (1.3.1)\n",
      "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Downloading sentence_transformers-5.2.2-py3-none-any.whl (494 kB)\n",
      "Downloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "   ---------------------------------------- 0.0/553.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 553.3/553.3 kB 3.2 MB/s eta 0:00:00\n",
      "Downloading torch-2.10.0-cp313-cp313-win_amd64.whl (113.8 MB)\n",
      "   ---------------------------------------- 0.0/113.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.8/113.8 MB 8.4 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 3.4/113.8 MB 8.4 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 5.2/113.8 MB 8.4 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 6.6/113.8 MB 7.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 8.1/113.8 MB 7.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 9.7/113.8 MB 7.7 MB/s eta 0:00:14\n",
      "   --- ------------------------------------ 11.0/113.8 MB 7.4 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 12.8/113.8 MB 7.5 MB/s eta 0:00:14\n",
      "   ---- ----------------------------------- 13.9/113.8 MB 7.5 MB/s eta 0:00:14\n",
      "   ----- ---------------------------------- 15.7/113.8 MB 7.4 MB/s eta 0:00:14\n",
      "   ------ --------------------------------- 17.3/113.8 MB 7.5 MB/s eta 0:00:13\n",
      "   ------ --------------------------------- 18.9/113.8 MB 7.5 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 20.2/113.8 MB 7.5 MB/s eta 0:00:13\n",
      "   ------- -------------------------------- 22.0/113.8 MB 7.5 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 23.1/113.8 MB 7.5 MB/s eta 0:00:13\n",
      "   -------- ------------------------------- 24.9/113.8 MB 7.5 MB/s eta 0:00:12\n",
      "   --------- ------------------------------ 26.5/113.8 MB 7.4 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 28.8/113.8 MB 7.6 MB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 30.9/113.8 MB 7.8 MB/s eta 0:00:11\n",
      "   ----------- ---------------------------- 33.0/113.8 MB 7.9 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 34.9/113.8 MB 7.9 MB/s eta 0:00:11\n",
      "   ------------ --------------------------- 36.7/113.8 MB 7.9 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 38.3/113.8 MB 7.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 40.1/113.8 MB 7.9 MB/s eta 0:00:10\n",
      "   -------------- ------------------------- 42.2/113.8 MB 8.0 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 44.0/113.8 MB 8.0 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 45.9/113.8 MB 8.1 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 47.7/113.8 MB 8.1 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 49.8/113.8 MB 8.2 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 51.9/113.8 MB 8.2 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 54.3/113.8 MB 8.3 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 56.1/113.8 MB 8.3 MB/s eta 0:00:07\n",
      "   -------------------- ------------------- 57.9/113.8 MB 8.4 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 59.8/113.8 MB 8.4 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 61.6/113.8 MB 8.4 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 63.4/113.8 MB 8.4 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 65.8/113.8 MB 8.5 MB/s eta 0:00:06\n",
      "   ----------------------- ---------------- 67.6/113.8 MB 8.5 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 69.5/113.8 MB 8.5 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 71.6/113.8 MB 8.5 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 73.7/113.8 MB 8.5 MB/s eta 0:00:05\n",
      "   -------------------------- ------------- 75.5/113.8 MB 8.6 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 76.8/113.8 MB 8.5 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 78.1/113.8 MB 8.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 80.0/113.8 MB 8.5 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 81.8/113.8 MB 8.5 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 82.8/113.8 MB 8.4 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 84.7/113.8 MB 8.4 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 86.2/113.8 MB 8.4 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 88.1/113.8 MB 8.4 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 89.9/113.8 MB 8.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 91.8/113.8 MB 8.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 93.8/113.8 MB 8.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 95.7/113.8 MB 8.5 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 98.6/113.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 99.9/113.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 102.0/113.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 103.5/113.8 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 105.6/113.8 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 107.0/113.8 MB 8.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 109.6/113.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  111.1/113.8 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  113.0/113.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  113.5/113.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  113.5/113.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  113.5/113.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  113.5/113.8 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 113.8/113.8 MB 8.0 MB/s eta 0:00:00\n",
      "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
      "   ---------------------------------------- 0.0/10.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 2.4/10.3 MB 11.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.9/10.3 MB 9.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.5/10.3 MB 8.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.1/10.3 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.9/10.3 MB 8.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.7/10.3 MB 7.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.2/10.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.3/10.3 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.6/2.9 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.9/2.9 MB 8.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.9/2.9 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.3/2.1 MB 6.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.8/2.1 MB 6.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading regex-2026.1.15-cp313-cp313-win_amd64.whl (277 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.3/6.3 MB 8.7 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.9/6.3 MB 8.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.2/6.3 MB 7.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.22.2-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.8/2.7 MB 9.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.6/2.7 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 6.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.20.3-py3-none-any.whl (16 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, shellingham, safetensors, regex, rank_bm25, networkx, hf-xet, fsspec, filelock, typer-slim, torch, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed filelock-3.20.3 fsspec-2026.2.0 hf-xet-1.2.0 huggingface-hub-1.4.1 mpmath-1.3.0 networkx-3.6.1 rank_bm25-0.2.2 regex-2026.1.15 safetensors-0.7.0 sentence-transformers-5.2.2 shellingham-1.5.4 sympy-1.14.0 tokenizers-0.22.2 torch-2.10.0 transformers-5.1.0 typer-slim-0.21.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install rank_bm25 sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1aea1b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import ipadic\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import japanize_matplotlib\n",
    "import os\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3ec2937f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_df = pd.read_csv(r'C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Data\\base_stories.tsv', sep='\\t')\n",
    "practice_df = pd.read_csv(r'C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Data\\fiction_stories_practice.tsv', sep='\\t')\n",
    "# --- 1. テストデータの読み込み ---\n",
    "test_df = pd.read_csv(r'C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Data\\fiction_stories_test.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da36ed66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeCabの初期化 (ipadicを使用)\n",
    "tagger = MeCab.Tagger(ipadic.MECAB_ARGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e5395781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(text):\n",
    "    \"\"\"文章から名詞・固有名詞を抽出する\"\"\"\n",
    "    if pd.isna(text): return []\n",
    "    node = tagger.parseToNode(text)\n",
    "    nouns = []\n",
    "    while node:\n",
    "        features = node.feature.split(',')\n",
    "        # 名詞かつ（一般、固有名詞、サ変接続）を対象とする\n",
    "        if features[0] == '名詞' and features[1] in ['一般', '固有名詞', 'サ変接続']:\n",
    "            if len(node.surface) > 1: # 1文字の記号などは除外\n",
    "                nouns.append(node.surface)\n",
    "        node = node.next\n",
    "    return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1bc7bfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'category', 'title', 'story'], dtype='object')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e4e6cb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 4)\n",
      "(20, 5)\n"
     ]
    }
   ],
   "source": [
    "print(base_df.shape)\n",
    "print(practice_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "8b8d4ab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>title_a</th>\n",
       "      <th>title_b</th>\n",
       "      <th>story</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>新幹線大爆破(1975)</td>\n",
       "      <td>フルメタル・ジャケット</td>\n",
       "      <td>大都市で相次いだ爆発により通信と電力が断たれ、交通網と物流は停止する。インフラの弱点が露わに...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>35</td>\n",
       "      <td>プライベート・ライアン</td>\n",
       "      <td>七人の侍</td>\n",
       "      <td>泥に沈む前線で、崩壊寸前の共同体を守る部隊に、敵地に取り残された通信員の救出命令が下る。だが...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>ジョーカー</td>\n",
       "      <td>FIRST SLAM DUNK</td>\n",
       "      <td>景気後退が続き失業者が増える街で、主人公は過去の事故が原因で夢を諦め、家族とも距離を置いてい...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>マイノリティ・リポート</td>\n",
       "      <td>インターステラー</td>\n",
       "      <td>重力異常が連鎖する未知の惑星へ、調査隊は自動運転車両で降下する。基地では「将来の映像を分析す...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>マトリックス</td>\n",
       "      <td>マイノリティ・リポート</td>\n",
       "      <td>街中や店の天井に据え付けられた小型の撮像装置が、人々の行動を絶えず記録する社会。集まった映像...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_a  id_b       title_a          title_b  \\\n",
       "0    29    23  新幹線大爆破(1975)      フルメタル・ジャケット   \n",
       "1     3    35   プライベート・ライアン             七人の侍   \n",
       "2    11    43         ジョーカー  FIRST SLAM DUNK   \n",
       "3    19     5   マイノリティ・リポート         インターステラー   \n",
       "4     2    19        マトリックス      マイノリティ・リポート   \n",
       "\n",
       "                                               story  \n",
       "0  大都市で相次いだ爆発により通信と電力が断たれ、交通網と物流は停止する。インフラの弱点が露わに...  \n",
       "1  泥に沈む前線で、崩壊寸前の共同体を守る部隊に、敵地に取り残された通信員の救出命令が下る。だが...  \n",
       "2  景気後退が続き失業者が増える街で、主人公は過去の事故が原因で夢を諦め、家族とも距離を置いてい...  \n",
       "3  重力異常が連鎖する未知の惑星へ、調査隊は自動運転車両で降下する。基地では「将来の映像を分析す...  \n",
       "4  街中や店の天井に据え付けられた小型の撮像装置が、人々の行動を絶えず記録する社会。集まった映像...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2b856326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_storiesの各作品からキーワードを抽出\n",
    "base_df['keywords'] = base_df['story'].apply(extract_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ff0d1ba7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>story</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>スター・ウォーズ エピソードIV/新たなる希望</td>\n",
       "      <td>銀河規模の専制国家が宇宙を支配し、巨大な軍事基地で各地の星々を脅している時代。反対勢力は地下...</td>\n",
       "      <td>[銀河, 規模, 専制, 国家, 宇宙, 支配, 軍事, 基地, 各地, 時代, 反対, 勢...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>マトリックス</td>\n",
       "      <td>昼は会社員、夜はハッカーとして生きるトーマス・アンダーソンは、自分の暮らす世界に強い違和感を...</td>\n",
       "      <td>[会社, ハッカー, トーマス, アンダーソン, 自分, 世界, 違和感, 高層, ビル, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>プライベート・ライアン</td>\n",
       "      <td>第二次世界大戦末期のヨーロッパ戦線。海を渡って上陸作戦が始まり、兵士たちは鉄の障害物が並ぶ海...</td>\n",
       "      <td>[世界, 大戦, 末期, ヨーロッパ, 戦線, 上陸, 作戦, 兵士, 障害, 海岸, 上陸...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>インセプション</td>\n",
       "      <td>近未来の企業社会では、人の意識に入り込んで情報を盗む犯罪が問題になっている。主人公のドム・コ...</td>\n",
       "      <td>[未来, 企業, 社会, 意識, 情報, 犯罪, 主人公, ドム・コブ, 装置, 眠り, 共...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>インターステラー</td>\n",
       "      <td>地球の近未来。異常気象と病害で作物が次々に枯れ、食料不足と砂嵐が日常になる。人々は宇宙開発へ...</td>\n",
       "      <td>[地球, 未来, 気象, 病害, 作物, 食料, 不足, 砂嵐, 日常, 人々, 宇宙, 開...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     title                                              story  \\\n",
       "0  スター・ウォーズ エピソードIV/新たなる希望  銀河規模の専制国家が宇宙を支配し、巨大な軍事基地で各地の星々を脅している時代。反対勢力は地下...   \n",
       "1                   マトリックス  昼は会社員、夜はハッカーとして生きるトーマス・アンダーソンは、自分の暮らす世界に強い違和感を...   \n",
       "2              プライベート・ライアン  第二次世界大戦末期のヨーロッパ戦線。海を渡って上陸作戦が始まり、兵士たちは鉄の障害物が並ぶ海...   \n",
       "3                  インセプション  近未来の企業社会では、人の意識に入り込んで情報を盗む犯罪が問題になっている。主人公のドム・コ...   \n",
       "4                 インターステラー  地球の近未来。異常気象と病害で作物が次々に枯れ、食料不足と砂嵐が日常になる。人々は宇宙開発へ...   \n",
       "\n",
       "                                            keywords  \n",
       "0  [銀河, 規模, 専制, 国家, 宇宙, 支配, 軍事, 基地, 各地, 時代, 反対, 勢...  \n",
       "1  [会社, ハッカー, トーマス, アンダーソン, 自分, 世界, 違和感, 高層, ビル, ...  \n",
       "2  [世界, 大戦, 末期, ヨーロッパ, 戦線, 上陸, 作戦, 兵士, 障害, 海岸, 上陸...  \n",
       "3  [未来, 企業, 社会, 意識, 情報, 犯罪, 主人公, ドム・コブ, 装置, 眠り, 共...  \n",
       "4  [地球, 未来, 気象, 病害, 作物, 食料, 不足, 砂嵐, 日常, 人々, 宇宙, 開...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_df[['title', 'story', 'keywords']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d096219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_keyword_retention(row, base_df):\n",
    "    \"\"\"\n",
    "    合成されたあらすじの中に、元ネタ(id_a, id_b)の単語が\n",
    "    「そのまま」何％含まれているかを計算する\n",
    "    \"\"\"\n",
    "    practice_story = row['story']\n",
    "    results = {}\n",
    "    \n",
    "    for suffix in ['a', 'b']:\n",
    "        target_id = row[f'id_{suffix}']\n",
    "        # 元ネタのキーワード集合\n",
    "        target_keywords = set(base_df[base_df['id'] == target_id]['keywords'].iloc[0])\n",
    "        \n",
    "        if not target_keywords:\n",
    "            results[f'retention_{suffix}'] = 0\n",
    "            continue\n",
    "            \n",
    "        # practiceの文章中に含まれているかチェック\n",
    "        found_count = sum(1 for word in target_keywords if word in practice_story)\n",
    "        results[f'retention_{suffix}'] = found_count / len(target_keywords)\n",
    "        \n",
    "    return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fd9fa20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スコアリング実行\n",
    "retention_df = practice_df.apply(lambda r: score_keyword_retention(r, base_df), axis=1)\n",
    "analysis_df = pd.concat([practice_df[['id_a', 'id_b']], retention_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "61977793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- キーワード残存率分析 ---\n",
      "       retention_a  retention_b\n",
      "count    20.000000    20.000000\n",
      "mean      0.110434     0.118066\n",
      "std       0.039437     0.047186\n",
      "min       0.049383     0.048077\n",
      "25%       0.081381     0.073279\n",
      "50%       0.108613     0.119117\n",
      "75%       0.133112     0.152098\n",
      "max       0.185185     0.227273\n"
     ]
    }
   ],
   "source": [
    "print(\"--- キーワード残存率分析 ---\")\n",
    "print(analysis_df[['retention_a', 'retention_b']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0a83833d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可視化\n",
    "# analysis_df[['retention_a', 'retention_b']].plot(kind='box', title='Keyword Retention Rate')\n",
    "# plt.ylabel('Retention Rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "65652355",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=extract_nouns, token_pattern=None)\n",
    "tfidf_matrix = vectorizer.fit_transform(base_df['story'])\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d94fa8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_rare_word_survival(row, base_df, tfidf_matrix, feature_names, top_n=10):\n",
    "    results = []\n",
    "    for suffix in ['a', 'b']:\n",
    "        target_id = row[f'id_{suffix}']\n",
    "        idx = base_df[base_df['id'] == target_id].index[0]\n",
    "        \n",
    "        # TF-IDF上位N個の「その作品らしい単語」を抽出\n",
    "        row_data = tfidf_matrix.getrow(idx).toarray().flatten()\n",
    "        top_indices = row_data.argsort()[-top_n:]\n",
    "        rare_words = [feature_names[i] for i in top_indices]\n",
    "        \n",
    "        # 合成文に含まれているか\n",
    "        found = [w for w in rare_words if w in row['story']]\n",
    "        results.append(len(found) / top_n)\n",
    "    return pd.Series(results, index=['rare_retention_a', 'rare_retention_b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b1204ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_analysis = practice_df.apply(lambda r: analyze_rare_word_survival(r, base_df, tfidf_matrix, feature_names), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fcce0a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- レア単語（特徴語）の生存率 ---\n",
      "       rare_retention_a  rare_retention_b\n",
      "count          20.00000         20.000000\n",
      "mean            0.16500          0.200000\n",
      "std             0.11821          0.152177\n",
      "min             0.00000          0.000000\n",
      "25%             0.10000          0.100000\n",
      "50%             0.20000          0.150000\n",
      "75%             0.20000          0.300000\n",
      "max             0.50000          0.600000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- レア単語（特徴語）の生存率 ---\")\n",
    "print(rare_analysis.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "435c3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_story_flow(sample_idx, save_dir=\"story_flows\"):\n",
    "    # 保存用ディレクトリの作成\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    row = practice_df.iloc[sample_idx]\n",
    "    sentences = [s for s in re.split(r'(?<=。)', row['story']) if s]\n",
    "    \n",
    "    # ベクトル計算\n",
    "    sent_vectors = vectorizer.transform([\" \".join(extract_nouns(s)) for s in sentences])\n",
    "    sims = cosine_similarity(sent_vectors, tfidf_matrix)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # 正解の2作品への類似度推移\n",
    "    idx_a = base_df[base_df['id'] == row['id_a']].index[0]\n",
    "    idx_b = base_df[base_df['id'] == row['id_b']].index[0]\n",
    "    \n",
    "    plt.plot(sims[:, idx_a], marker='o', label=f\"Target A (ID:{row['id_a']})\", color='blue')\n",
    "    plt.plot(sims[:, idx_b], marker='s', label=f\"Target B (ID:{row['id_b']})\", color='red')\n",
    "    \n",
    "    # ノイズの最大値\n",
    "    mask = np.ones(sims.shape[1], dtype=bool)\n",
    "    mask[[idx_a, idx_b]] = False\n",
    "    max_noise = sims[:, mask].max(axis=1)\n",
    "    plt.plot(max_noise, label=\"Max Noise\", color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.title(f\"Story Flow Analysis: Practice ID {sample_idx}\")\n",
    "    plt.xlabel(\"Sentence Position\")\n",
    "    plt.ylabel(\"Similarity Score\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # --- 保存処理 ---\n",
    "    save_path = os.path.join(save_dir, f\"flow_{sample_idx:03d}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close() # メモリ解放のためにcloseする\n",
    "    print(f\"Saved: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "27aaf5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_000.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_001.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_002.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_003.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_004.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_005.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_006.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_007.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_008.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_009.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_010.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_011.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_012.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_013.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_014.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_015.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_016.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_017.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_018.png\n",
      "Saved: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\\flow_019.png\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 文字列をPathオブジェクトに変換\n",
    "output_path = Path(r\"C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\residual_plot\")\n",
    "\n",
    "for i in range(20):\n",
    "    plot_story_flow(i, save_dir=str(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "897c143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. BM25 の準備 ---\n",
    "# base_stories の全ストーリーをトークン化（名詞抽出）してインデックス作成\n",
    "tokenized_base = [extract_nouns(s) for s in base_df['story']]\n",
    "bm25 = BM25Okapi(tokenized_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "781a836b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61c7b12d67b4584b1454f69e662a24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/387 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\管理\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\管理\\.cache\\huggingface\\hub\\models--intfloat--multilingual-e5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759d9fdb517c4e23b444fee9807f6fbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7911f088723e4482938707dfeb371bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d312577f6444509ccdb6c4ba7b8377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3222a9bf79487c93973248a8da0e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33119bbd8f20457188679352642ac8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: intfloat/multilingual-e5-small\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000069505a984a95b497105b24559080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/443 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bbb45fcd184ff58bd2e6878576f09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28f1213cd5d140dc84af1df2af75c4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a6adbd88d9a448cad9f52e45fe92421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 2. ベクトル検索 (Sentence-Transformers) の準備 ---\n",
    "# 日本語に強く、軽量なモデルを使用します\n",
    "model_name = 'intfloat/multilingual-e5-small'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "37a72b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_stories をベクトル化 (E5モデルの形式に従い \"passage: \" を付与)\n",
    "base_texts = [\"passage: \" + s for s in base_df['story']]\n",
    "base_embeddings = model.encode(base_texts, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "65031d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sliding_windows(text, window_size=2):\n",
    "    \"\"\"文章を指定した文数の窓で分割する\"\"\"\n",
    "    sentences = [s.strip() + \"。\" for s in re.split(r'(?<=。)', text) if s.strip()]\n",
    "    if len(sentences) <= window_size:\n",
    "        return [text]\n",
    "    \n",
    "    windows = []\n",
    "    for i in range(len(sentences) - window_size + 1):\n",
    "        windows.append(\"\".join(sentences[i:i + window_size]))\n",
    "    return windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "22ec17f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_window_search(query_story, k=60):\n",
    "    \"\"\"\n",
    "    窓ごとに BM25 と Vector 検索を行い、RRF で統合。\n",
    "    各作品について、全窓の中での『最高順位』を最終的な順位として採用する。\n",
    "    \"\"\"\n",
    "    windows = get_sliding_windows(query_story, window_size=2) # 2文ずつの窓\n",
    "    num_docs = len(base_df)\n",
    "    \n",
    "    # 各作品ID(index)ごとの最小順位（最高順位）を保持する\n",
    "    # 初期値は最下位（順位 = ドキュメント数）\n",
    "    best_bm25_ranks = np.full(num_docs, num_docs)\n",
    "    best_vec_ranks = np.full(num_docs, num_docs)\n",
    "\n",
    "    for window in windows:\n",
    "        # --- BM25 検索 ---\n",
    "        q_tokens = extract_nouns(window)\n",
    "        if q_tokens:\n",
    "            bm25_scores = bm25.get_scores(q_tokens)\n",
    "            # 順位に変換 (値が大きい順に 1, 2, 3...)\n",
    "            ranks = np.argsort(np.argsort(bm25_scores)[::-1]) + 1\n",
    "            best_bm25_ranks = np.minimum(best_bm25_ranks, ranks)\n",
    "\n",
    "        # --- Vector 検索 ---\n",
    "        q_emb = model.encode([\"query: \" + window], normalize_embeddings=True)\n",
    "        vec_sims = cosine_similarity(q_emb, base_embeddings)[0]\n",
    "        # 順位に変換\n",
    "        ranks = np.argsort(np.argsort(vec_sims)[::-1]) + 1\n",
    "        best_vec_ranks = np.minimum(best_vec_ranks, ranks)\n",
    "\n",
    "    # --- RRF (Reciprocal Rank Fusion) による統合スコアリング ---\n",
    "    # 各作品が、いずれかの窓で「BM25上位」または「Vector上位」に入っていれば高スコアになる\n",
    "    rrf_scores = (1.0 / (k + best_bm25_ranks)) + (1.0 / (k + best_vec_ranks))\n",
    "    \n",
    "    # スコア順にソートした結果を返す\n",
    "    top_indices = np.argsort(rrf_scores)[::-1]\n",
    "    \n",
    "    results = base_df.iloc[top_indices].copy()\n",
    "    results['search_score'] = rrf_scores[top_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "91371231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search(df, top_n=10):\n",
    "    hits = 0\n",
    "    total_relevant = len(df) * 2\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        preds = hybrid_window_search(row['story'])\n",
    "        top_ids = preds['id'].head(top_n).values\n",
    "        \n",
    "        # 正解が含まれているかチェック\n",
    "        if row['id_a'] in top_ids: hits += 1\n",
    "        if row['id_b'] in top_ids: hits += 1\n",
    "            \n",
    "    recall_at_n = hits / total_relevant\n",
    "    print(f\"Recall@{top_n}: {recall_at_n:.2%} ({hits}/{total_relevant})\")\n",
    "    return recall_at_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "226dc7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on practice_df...\n",
      "Recall@10: 85.00% (34/40)\n"
     ]
    }
   ],
   "source": [
    "# 練習用データ 20件で検証\n",
    "print(\"Evaluating on practice_df...\")\n",
    "recall = evaluate_search(practice_df, top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c5e3aaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_search_details(df, top_n=10):\n",
    "    detailed_results = []\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        preds = hybrid_window_search(row['story'])\n",
    "        # 全作品の中での正解の順位を取得 (indexは0始まりなので+1する)\n",
    "        rank_a = np.where(preds['id'].values == row['id_a'])[0][0] + 1\n",
    "        rank_b = np.where(preds['id'].values == row['id_b'])[0][0] + 1\n",
    "        \n",
    "        detailed_results.append({\n",
    "            'practice_id': i,\n",
    "            'id_a': row['id_a'],\n",
    "            'rank_a': rank_a,\n",
    "            'hit_a': rank_a <= top_n,\n",
    "            'id_b': row['id_b'],\n",
    "            'rank_b': rank_b,\n",
    "            'hit_b': rank_b <= top_n,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0ccf4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行\n",
    "check_df = check_search_details(practice_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d9e9bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正解が10位以内に入らなかったケース ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>practice_id</th>\n",
       "      <th>id_a</th>\n",
       "      <th>rank_a</th>\n",
       "      <th>hit_a</th>\n",
       "      <th>id_b</th>\n",
       "      <th>rank_b</th>\n",
       "      <th>hit_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>18</td>\n",
       "      <td>26</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>40</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    practice_id  id_a  rank_a  hit_a  id_b  rank_b  hit_b\n",
       "4             4     2      28  False    19       1   True\n",
       "7             7    24       4   True    18      26  False\n",
       "8             8    13      13  False    28      20  False\n",
       "10           10    41       1   True    10      11  False\n",
       "13           13    40       9   True    50      14  False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. データの抽出\n",
    "missed_cases = check_df[(check_df['rank_a'] > 10) | (check_df['rank_b'] > 10)]\n",
    "\n",
    "# 2. 画面表示（Jupyter上で表が見えるようになります）\n",
    "print(\"--- 正解が10位以内に入らなかったケース ---\")\n",
    "display(missed_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8a1dd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CSVへの保存（missed_cases に対して実行する）\n",
    "save_path = r'C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\accuracy_not_10.csv'\n",
    "missed_cases.to_csv(save_path, index=False) # index=False をつけると余計なID列が入りません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8dc6991d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正解順位の統計 ---\n",
      "          rank_a     rank_b\n",
      "count  20.000000  20.000000\n",
      "mean    4.300000   5.550000\n",
      "std     6.366855   7.029824\n",
      "min     1.000000   1.000000\n",
      "25%     1.000000   1.000000\n",
      "50%     2.000000   2.500000\n",
      "75%     4.000000   6.500000\n",
      "max    28.000000  26.000000\n"
     ]
    }
   ],
   "source": [
    "# 全体の順位の分布を確認\n",
    "print(\"\\n--- 正解順位の統計 ---\")\n",
    "print(check_df[['rank_a', 'rank_b']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f998492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Statistics_on_Correct_Answer_Rankings = check_df[['rank_a', 'rank_b']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "917cf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. CSVへの保存（missed_cases に対して実行する）\n",
    "save_path = r'C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\Statistics_on_Correct_Answer_Rankings.csv'\n",
    "Statistics_on_Correct_Answer_Rankings.to_csv(save_path, index=False) # index=False をつけると余計なID列が入りません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "abf49b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting predictions for test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 340/340 [00:51<00:00,  6.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- 2. 予測の実行 ---\n",
    "# ※ 340件の処理には数分かかる場合があります\n",
    "results = []\n",
    "\n",
    "print(\"Starting predictions for test data...\")\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    test_id = row['id']\n",
    "    story_text = row['story']\n",
    "    \n",
    "    # 前のステップで作成したハイブリッド検索を実行\n",
    "    search_results = hybrid_window_search(story_text)\n",
    "    \n",
    "    # 上位2件の作品IDを取得\n",
    "    # 本来はここで検索上位10~30件を LLM に渡し、2つに絞り込むリランキングを行う\n",
    "    top_2_ids = search_results['id'].head(2).values\n",
    "    \n",
    "    # 戦略に基づき、予測した2つのIDを昇順（小さい順）に並べ替え\n",
    "    pred1, pred2 = sorted(top_2_ids)\n",
    "    \n",
    "    results.append([test_id, pred1, pred2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a012e9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df = pd.DataFrame(results, columns=['ID', 'Pred1', 'Pred2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "03d96530",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = r'C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\submission.csv'\n",
    "submit_df.to_csv(output_filename, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "949a0667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully created: C:\\Users\\管理\\Documents\\GitHub\\Data-Analysis_competition\\Analysis\\sigante_anime\\Analysis_file_3rd\\out_put\\submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Pred1</th>\n",
       "      <th>Pred2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Pred1  Pred2\n",
       "0   1     26     27\n",
       "1   2      8     34\n",
       "2   3     33     37\n",
       "3   4     26     36\n",
       "4   5     17     18"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"\\nSuccessfully created: {output_filename}\")\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "430efd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(df, top_n=10):\n",
    "    perfect_matches = 0  # 2つとも正解（順不同）\n",
    "    partial_matches = 0  # 1つだけ正解\n",
    "    both_in_top_n = 0    # 2つとも上位N件に入っている\n",
    "    any_in_top_n = 0     # 少なくとも1つが上位N件に入っている\n",
    "    \n",
    "    total_cases = len(df)\n",
    "    \n",
    "    print(f\"Calculating metrics for {total_cases} cases...\")\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        # 検索の実行\n",
    "        preds = hybrid_window_search(row['story'])\n",
    "        top_ids = preds['id'].values\n",
    "        \n",
    "        # 1. 完全一致の判定 (Top 2 が正解ペアと一致するか)\n",
    "        predicted_top_2 = set(top_ids[:2])\n",
    "        true_ids = {row['id_a'], row['id_b']}\n",
    "        \n",
    "        if predicted_top_2 == true_ids:\n",
    "            perfect_matches += 1\n",
    "        \n",
    "        # 1つだけ正解しているか（デバッグ用）\n",
    "        match_count_top_2 = len(predicted_top_2.intersection(true_ids))\n",
    "        if match_count_top_2 == 1:\n",
    "            partial_matches += 1\n",
    "\n",
    "        # 2. 検索エンジンの性能判定 (Top 10 に正解が含まれているか)\n",
    "        search_top_n = set(top_ids[:top_n])\n",
    "        match_count_top_n = len(search_top_n.intersection(true_ids))\n",
    "        \n",
    "        if match_count_top_n == 2:\n",
    "            both_in_top_n += 1\n",
    "        if match_count_top_n >= 1:\n",
    "            any_in_top_n += 1\n",
    "\n",
    "    # 指標の計算\n",
    "    accuracy = perfect_matches / total_cases\n",
    "    recall_both = both_in_top_n / total_cases\n",
    "    \n",
    "    print(\"\\n--- 評価結果 ---\")\n",
    "    print(f\"1. 完全一致正解率 (Perfect Match Accuracy): {accuracy:.2%} ({perfect_matches}/{total_cases})\")\n",
    "    print(f\"   ※ 上位2件が正解の2つと完全に一致した割合\")\n",
    "    print(f\"\\n2. Top-{top_n} 両方包含率 (Both in Top-{top_n}): {recall_both:.2%} ({both_in_top_n}/{total_cases})\")\n",
    "    print(f\"   ※ 上位{top_n}件の中に、正解の2つが両方とも含まれていた割合\")\n",
    "    print(f\"\\n3. 参考指標:\")\n",
    "    print(f\"   - 少なくとも1つ正解 (Top 2): {(perfect_matches + partial_matches)/total_cases:.2%}\")\n",
    "    print(f\"   - 少なくとも1つ包含 (Top {top_n}): {any_in_top_n/total_cases:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a1cc5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating metrics for 20 cases...\n",
      "\n",
      "--- 評価結果 ---\n",
      "1. 完全一致正解率 (Perfect Match Accuracy): 30.00% (6/20)\n",
      "   ※ 上位2件が正解の2つと完全に一致した割合\n",
      "\n",
      "2. Top-10 両方包含率 (Both in Top-10): 75.00% (15/20)\n",
      "   ※ 上位10件の中に、正解の2つが両方とも含まれていた割合\n",
      "\n",
      "3. 参考指標:\n",
      "   - 少なくとも1つ正解 (Top 2): 80.00%\n",
      "   - 少なくとも1つ包含 (Top 10): 95.00%\n"
     ]
    }
   ],
   "source": [
    "# 実行\n",
    "calculate_metrics(practice_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd43039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3728628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
